{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME Explanation for Hypertension Detection\n",
    "In this notebook, we will walk through interpreting a machine learning problem (predicting the risk of developing hypertension) using records of the Cardiorespiratory Fitness dataset from the Henry Ford Testing (FIT) Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HTN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23095 entries, 0 to 23094\n",
      "Data columns (total 50 columns):\n",
      "studycode                         23095 non-null int64\n",
      "age                               23095 non-null float64\n",
      "sex                               23095 non-null object\n",
      "bmi                               13543 non-null float64\n",
      "mets_achieved                     23095 non-null float64\n",
      "mets_c                            23095 non-null int64\n",
      "resting_heart_rt                  23095 non-null int64\n",
      "resting_systolic                  23095 non-null int64\n",
      "resting_diastolic                 23095 non-null int64\n",
      "peak_heart_rt                     23095 non-null int64\n",
      "peak_systolic                     23057 non-null float64\n",
      "peak_diastolic                    23023 non-null float64\n",
      "percent_hr_achieved               23095 non-null float64\n",
      "adequatestudy85                   23095 non-null int64\n",
      "0rm_rest_ekg_flg                  20962 non-null float64\n",
      "reason                            23095 non-null object\n",
      "aspirin                           23095 non-null int64\n",
      "statin                            23095 non-null int64\n",
      "other_lipid                       23095 non-null int64\n",
      "all_lipid                         23095 non-null int64\n",
      "insulin                           23095 non-null int64\n",
      "dmmed                             23095 non-null int64\n",
      "depressionmed                     23095 non-null int64\n",
      "smokingmeds                       23095 non-null int64\n",
      "plavix                            23095 non-null int64\n",
      "nitrates                          23095 non-null int64\n",
      "edmeds                            23095 non-null int64\n",
      "dig                               23095 non-null int64\n",
      "lungdiseasemeds                   23095 non-null int64\n",
      "dm                                23095 non-null int64\n",
      "hyperlipid                        23095 non-null int64\n",
      "famhx                             23095 non-null int64\n",
      "oldchf                            23095 non-null int64\n",
      "priorCVA                          23095 non-null int64\n",
      "smoke                             23095 non-null int64\n",
      "k0wncad                           23095 non-null int64\n",
      "mibefore                          23095 non-null int64\n",
      "cabgbefore                        23095 non-null int64\n",
      "pcibefore                         23095 non-null int64\n",
      "afib2                             23095 non-null int64\n",
      "aflutter2                         23095 non-null int64\n",
      "warfarin2                         23095 non-null int64\n",
      "obesity2                          23095 non-null int64\n",
      "sedentary2                        23095 non-null int64\n",
      "htnresponse                       23095 non-null int64\n",
      "black                             23095 non-null int64\n",
      "Model from logisitc regression    23095 non-null float64\n",
      "Framingham risk                   23095 non-null float64\n",
      "CVDrisk                           23094 non-null float64\n",
      "newHTN                            23095 non-null int64\n",
      "dtypes: float64(10), int64(38), object(2)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_idx = df[df['peak_diastolic'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['peak_diastolic'][df['peak_diastolic'].isnull()] = df['peak_diastolic'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[null_idx[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1 = df.quantile(q=0.25)\n",
    "df_q3 = df.quantile(q=0.75)\n",
    "df_iqr = df_q3 - df_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'mets_achieved', 'resting_systolic', 'resting_diastolic',\n",
    "       'peak_diastolic', 'hyperlipid', 'percent_hr_achieved']\n",
    "for col_name in numeric_cols:\n",
    "    col_mask = (df[col_name] >= (df_q1[col_name] - (1.5 * df_iqr[col_name]))) & (df[col_name] <= (df_q3[col_name] + (1.5 * df_iqr[col_name])))\n",
    "    df[col_name] = df[col_name][col_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select used features and target from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select used columns only (features and target)\n",
    "df_all = df[[\"age\",\"mets_achieved\",\"resting_systolic\",\"resting_diastolic\",\"peak_diastolic\",\"reason\",\"htnresponse\",\"k0wncad\",\"dm\",\"aspirin\",\"hyperlipid\",\"black\",\"percent_hr_achieved\",\"newHTN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: number of records after removing outliers = 21515 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_cols = [\"age\",\"mets_achieved\",\"resting_systolic\",\"resting_diastolic\",\"peak_diastolic\",\"percent_hr_achieved\"]\n",
    "quantile_labels = [1, 2, 3, 4]\n",
    "quantile_list = [0, .25, .5, .75, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in disc_cols:\n",
    "    df_all[col_name] = pd.qcut(df_all[col_name], labels= quantile_labels, q=quantile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a label encoding scheme for the 'reason' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chest Pain', 'Pre-Operation', 'Ab0rmal Test', 'Rule out Ischemia',\n",
       "       'K0wn CAD', 'Shortness of Breath', 'Other', 'Arrhythmia',\n",
       "       'Palpitation', 'Screening, Research', 'Dizzy, Fatigue',\n",
       "       'Risk Factor', 'Conduction System Disease'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.reason.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ab0rmal Test',\n",
       " 1: 'Arrhythmia',\n",
       " 2: 'Chest Pain',\n",
       " 3: 'Conduction System Disease',\n",
       " 4: 'Dizzy, Fatigue',\n",
       " 5: 'K0wn CAD',\n",
       " 6: 'Other',\n",
       " 7: 'Palpitation',\n",
       " 8: 'Pre-Operation',\n",
       " 9: 'Risk Factor',\n",
       " 10: 'Rule out Ischemia',\n",
       " 11: 'Screening, Research',\n",
       " 12: 'Shortness of Breath'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "reason_labels = le.fit_transform(df_all['reason'])\n",
    "reason_mappings = {index: label for index, label in enumerate(le.classes_)}\n",
    "reason_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['reason'] = reason_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23095 entries, 0 to 23094\n",
      "Data columns (total 14 columns):\n",
      "age                    23095 non-null category\n",
      "mets_achieved          23095 non-null category\n",
      "resting_systolic       23095 non-null category\n",
      "resting_diastolic      23095 non-null category\n",
      "peak_diastolic         23095 non-null category\n",
      "reason                 23095 non-null int64\n",
      "htnresponse            23095 non-null int64\n",
      "k0wncad                23095 non-null int64\n",
      "dm                     23095 non-null int64\n",
      "aspirin                23095 non-null int64\n",
      "hyperlipid             23095 non-null int64\n",
      "black                  23095 non-null int64\n",
      "percent_hr_achieved    23095 non-null category\n",
      "newHTN                 23095 non-null int64\n",
      "dtypes: category(6), int64(8)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Used Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats = df_all[[\"age\",\"mets_achieved\",\"resting_systolic\",\"resting_diastolic\",\"peak_diastolic\",\"reason\",\"htnresponse\",\"k0wncad\",\"dm\",\"aspirin\",\"hyperlipid\",\"black\",\"percent_hr_achieved\"]]\n",
    "df_target = df_all[[\"newHTN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling class imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.newHTN.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of positive class instances = 7302, and negative class instances = 14213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "X, y = df_feats, df_target\n",
    "print('Original dataset shape {}'.format(Counter(y['newHTN'])))\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_res, y_res = sm.fit_sample(X, y)\n",
    "X_resampled = pd.DataFrame(X_res)\n",
    "X_resampled.columns = X.columns\n",
    "y_resampled = pd.DataFrame(y_res)\n",
    "y_resampled.columns = y.columns\n",
    "\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover labels for categorical attributes after resampling\n",
    "X_resampled = X_resampled.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 70% training and 30% testing set\n",
    "X, X_test, y, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16166, 13)\n",
      "(6929, 13)\n",
      "(16166, 1)\n",
      "(6929, 1)\n"
     ]
    }
   ],
   "source": [
    "# without SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 70% training and 30% testing set\n",
    "X, X_test, y, y_test = train_test_split(df_feats, df_target, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error\n",
    "def mean_abs_error(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline guess is a score of 0.34\n",
      "Baseline Performance on the test set: MAE = 0.4575\n"
     ]
    }
   ],
   "source": [
    "baseline_guess = np.mean(y)\n",
    "\n",
    "print('The baseline guess is a score of %0.2f' % baseline_guess)\n",
    "print(\"Baseline Performance on the test set: MAE = %0.4f\" % mean_abs_error(y_test, baseline_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive method of guessing the median training value provides us a low baseline for our models to beat! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "X.to_csv('data/training_features.csv', index = False)\n",
    "X_test.to_csv('data/testing_features.csv', index = False)\n",
    "y.to_csv('data/training_labels.csv', index = False)\n",
    "y_test.to_csv('data/testing_labels.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No SMOTE\n",
    "X.to_csv('data/training_features_noSMOTE.csv', index = False)\n",
    "X_test.to_csv('data/testing_features_noSMOTE.csv', index = False)\n",
    "y.to_csv('data/training_labels_noSMOTE.csv', index = False)\n",
    "y_test.to_csv('data/testing_labels_noSMOTE.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models\n",
    "The data will be trained using three models (ANN, Random Forest and SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X, y to arrays (vector)\n",
    "X = np.array(X)\n",
    "X_test = np.array(X_test)\n",
    "y = np.array(y).reshape((-1, ))\n",
    "y_test = np.array(y_test).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mean_absolute_error(y_test, model_pred)\n",
    "    acc = accuracy_score(y_test, model_pred)\n",
    "    # Return the performance metric\n",
    "    return model_mae, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer perceptron classifier\n",
    "ANN = MLPClassifier()\n",
    "ANN_mae, accuracy = fit_and_evaluate(ANN)\n",
    "\n",
    "print('Multilayer Perceptron Classifier Performance on the test set: MAE = %0.4f' % ANN_mae)\n",
    "print('Accuracy: %0.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Performance on the test set: MAE = 0.3647\n",
      "Accuracy: 0.6353\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "rtf = RandomForestClassifier(n_estimators= 50)\n",
    "rtf_mae, accuracy = fit_and_evaluate(rtf)\n",
    "\n",
    "print('Random Forest Classifier Performance on the test set: MAE = %0.4f' % rtf_mae)\n",
    "print('Accuracy: %0.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine classifier\n",
    "svm = SVC()\n",
    "svm_mae, accuracy = fit_and_evaluate(svm)\n",
    "\n",
    "print('Support Vector Machine Classifier Performance on the test set: MAE = %0.4f' % svm_mae)\n",
    "print('Accuracy: %0.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong Predictions Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the wrongly classified instances (False positive and false negative instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_binary = rtf.predict(X_test)\n",
    "preds_prob = rtf.predict_proba(X_test)\n",
    "\n",
    "# find correct and wrong predictions\n",
    "abs_diff = abs(preds_binary - y_test)\n",
    "    \n",
    "wrong_instances = X_test[abs_diff==1,:]\n",
    "right_instances = X_test[abs_diff==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2527"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6929"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Cutoff Probability\n",
    "Make sure that the cutoff probability is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6929"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_half = []\n",
    "for p in preds_prob:\n",
    "    if p[0] >= 0.5:\n",
    "        preds_half.append(0)\n",
    "    else:\n",
    "        preds_half.append(1)\n",
    "        \n",
    "compare = (preds_half == preds_binary)*1\n",
    "len(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True positive: Sick people correctly identified as sick\n",
    "# False positive: Healthy people incorrectly identified as sick\n",
    "# True negative: Healthy people correctly identified as healthy\n",
    "# False negative: Sick people incorrectly identified as healthy\n",
    "    \n",
    "def categories(y_actual, y_hat):\n",
    "    TP = []\n",
    "    FP = []\n",
    "    TN = []\n",
    "    FN = []\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i] == y_hat[i] == 1:\n",
    "            TP.append(i)\n",
    "        if y_hat[i] == 1  and y_actual[i] != y_hat[i]:\n",
    "            FP.append(i)\n",
    "        if y_actual[i] == y_hat[i] == 0:\n",
    "            TN.append(i)\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN.append(i)\n",
    "\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the test set instances as the four (True positive, true negative, false positive, false negative)\n",
    "tp_idx, fp_idx, tn_idx, fn_idx = categories(y_test, preds_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of wrong instances matched the output\n",
    "len(wrong_instances) == len(fp_idx) + len(fn_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the False instances by their probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the probabilities of the right class of the FP and FN instances\n",
    "fp_probs = []\n",
    "fn_probs = []\n",
    "\n",
    "for idx in fp_idx:\n",
    "        fp_probs.append(preds_prob[idx][0])\n",
    "        \n",
    "for idx in fn_idx:\n",
    "        fn_probs.append(preds_prob[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(fp_probs, fp_idx)\n",
    "fp_sorted  = sorted(zipped)\n",
    "\n",
    "zipped = zip(fn_probs, fn_idx)\n",
    "fn_sorted  = sorted(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Sorted Probabilities\n",
    "Each of fp_sorted and fn_sorted has list of tuples (probability of their right class, instance's index in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn = pd.DataFrame(fn_sorted, columns=['right_prob', 'test_idx'])\n",
    "df_fp = pd.DataFrame(fp_sorted, columns=['right_prob', 'test_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn.to_csv('data/fn_noSMOTE.csv', index = False)\n",
    "df_fp.to_csv('data/fp_noSMOTE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the RTF model with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rtf.predict(X_test)\n",
    "\n",
    "# find correct and wrong predictions\n",
    "abs_diff = abs(preds - y_test)\n",
    "    \n",
    "# Exact the worst and best prediction\n",
    "wrong = X_test[np.argmax(abs_diff), :]\n",
    "correct = X_test[np.argmin(abs_diff), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME for explaining predictions\n",
    "import lime \n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lime explainer object\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data = X, \n",
    "                                                   mode = 'classification',\n",
    "                                                   training_labels = y,\n",
    "                                                   feature_names = df_feats.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predicted and true value for the wrong example\n",
    "print('Prediction: %d' % rtf.predict(wrong.reshape(1, -1)))\n",
    "print('Actual Value: %d' % y_test[np.argmax(abs_diff)])\n",
    "\n",
    "# Explanation for wrong prediction\n",
    "wrong_exp = explainer.explain_instance(data_row = wrong, \n",
    "                                       predict_fn = rtf.predict_proba)\n",
    "\n",
    "# Plot the prediction explaination\n",
    "wrong_exp.as_pyplot_figure();\n",
    "plt.title('Explanation of Wrong Prediction', size = 28);\n",
    "plt.xlabel('Effect on Prediction', size = 22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong\n",
    "wrong_df = pd.Series(wrong, index=df_feats.columns )\n",
    "wrong_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predicted and true value for the wrong instance\n",
    "print('Prediction: %d' % rtf.predict(correct.reshape(1, -1)))\n",
    "print('Actual Value: %d' % y_test[np.argmin(abs_diff)])\n",
    "\n",
    "# Explanation for wrong prediction\n",
    "correct_exp = explainer.explain_instance(correct, rtf.predict_proba)\n",
    "correct_exp.as_pyplot_figure();\n",
    "plt.title('Explanation of Correct Prediction', size = 28);\n",
    "plt.xlabel('Effect on Prediction', size = 22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.Series(correct, index=df_feats.columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the RTF Model with Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_tabular\n",
    "from anchor import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = [X_resampled, y_resampled]\n",
    "# df_all_final = pd.concat(frames, axis=1)\n",
    "\n",
    "# without SMOTE\n",
    "frames = [df_feats, df_target]\n",
    "df_all_final = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_final.to_csv('data/dataset_all.csv', index = False)\n",
    "dataset_csv = 'data/dataset_all.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_all_final.columns.tolist()\n",
    "features_to_use = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "categorical_feats = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.load_csv_dataset(dataset_csv, target_idx= 13, delimiter=',',\n",
    "                             feature_names=feature_names, categorical_features=categorical_feats,\n",
    "                            feature_transformations=None, discretize=False, balance=False, fill_na='-1', skip_first=True,\n",
    "                                features_to_use=features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainerAnchor = anchor_tabular.AnchorTabularExplainer(dataset.class_names, dataset.feature_names, dataset.data, dataset.categorical_names)\n",
    "explainerAnchor.fit(dataset.train, dataset.labels_train, dataset.validation, dataset.labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtf.fit(explainerAnchor.encoder.transform(dataset.train), dataset.labels_train)\n",
    "predict_fn = lambda x: rtf.predict(explainerAnchor.encoder.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train', accuracy_score(dataset.labels_train, predict_fn(dataset.train)))\n",
    "print('Test', accuracy_score(dataset.labels_test, predict_fn(dataset.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an Anchor\n",
    "Below, we get an anchor for prediction number 0. An anchor is a sufficient condition - that is, when the anchor holds, the prediction should be the same as the prediction for this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "np.random.seed(1)\n",
    "desired_label = predict_fn(dataset.test[idx].reshape(1, -1))[0]\n",
    "print('Prediction: ', explainerAnchor.class_names[predict_fn(dataset.test[idx].reshape(1, -1))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainerAnchor.explain_instance(dataset.test[idx], rtf.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set threshold to 0.95, so we guarantee (with high probability) that precision will be above 0.95 - that is, that predictions on instances where the anchor holds will be the same as the original prediction at least 95% of the time. Let's try it out on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test examples where the anchora pplies\n",
    "fit_anchor = np.where(np.all(dataset.test[:, exp.features()] == dataset.test[idx][exp.features()], axis=1))[0]\n",
    "print('Anchor test coverage: %.2f' % (fit_anchor.shape[0] / float(dataset.test.shape[0])))\n",
    "print('Anchor test precision: %.2f' % (np.mean(predict_fn(dataset.test[fit_anchor]) == predict_fn(dataset.test[idx].reshape(1, -1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a Particular Anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at just part of the anchor - for example, the first two clauses. Note how these do not have enough precision, which is why the explainer added a third one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Partial anchor: %s' % (' AND '.join(exp.names(1))))\n",
    "print('Partial precision: %.2f' % exp.precision(1))\n",
    "print('Partial coverage: %.2f' % exp.coverage(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_partial = np.where(np.all(dataset.test[:, exp.features(1)] == dataset.test[idx][exp.features(1)], axis=1))[0]\n",
    "print('Partial anchor test precision: %.2f' % (np.mean(predict_fn(dataset.test[fit_partial]) == predict_fn(dataset.test[idx].reshape(1, -1)))))\n",
    "print('Partial anchor test coverage: %.2f' % (fit_partial.shape[0] / float(dataset.test.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See a visualization of the anchor with examples and etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
